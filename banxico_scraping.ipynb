{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, ElementClickInterceptedException\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "import pickle\n",
    "from IPython.display import display, clear_output\n",
    "from dotenv import load_dotenv, dotenv_values \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sector_ids(url):\n",
    "    \"\"\"Function to scrap sector IDs in elements with onclick property from Banxico.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        banxico url containig the series id information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sector_ids : list\n",
    "        list containing sector ids as strings\n",
    "    \"\"\"\n",
    "    sector_ids = []\n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(5)\n",
    "        \n",
    "        #driver.find_element_by_xpath(\".//input[contains(@onclick, '1 Bedroom Deluxe')]\")\n",
    "        elements = driver.find_elements(By.XPATH, '//*[@onclick]')\n",
    "\n",
    "        if len(elements) < 3: \n",
    "            driver.quit()\n",
    "            time.sleep(10)\n",
    "            find_sector_ids(url)\n",
    "\n",
    "\n",
    "        for element_id in elements:\n",
    "            onclick_txt = element_id.get_attribute('onclick')\n",
    "            if \"cargaDirectorioSector\" in onclick_txt:\n",
    "                sector_ids.append((onclick_txt.split(\",\")[0].split(\"(\")[-1]))\n",
    "        return sector_ids\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "banxico_series_url = \"https://www.banxico.org.mx/SieAPIRest/service/v1/doc/catalogoSeries\" \n",
    "\n",
    "sector_ids = find_sector_ids(banxico_series_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectores_api_url = f\"https://www.banxico.org.mx/SieAPIRest/service/v1/cat/sectores/\"\n",
    "def find_cuadroid_by_sectorid(sectors_ids):\n",
    "    \"\"\"Fucntion calls sectores benxico API to retrieve Sector IDs gievn a list of sector\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sector_ids : list\n",
    "        list containing strings for sector ids\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sector_ids : list\n",
    "        list containing strings for cuadro ids \n",
    "    \"\"\"\n",
    "    id_cuadros = []\n",
    "\n",
    "    def recursive_func(item):\n",
    "        if isinstance(item, dict):\n",
    "            if 'idCuadro' in item:\n",
    "                id_cuadros.append(item['idCuadro'])\n",
    "            if 'cuadros' in item:\n",
    "                recursive_func(item['cuadros'])\n",
    "        elif isinstance(item, list):\n",
    "            for i_t in item:\n",
    "                recursive_func(i_t)\n",
    "                \n",
    "    for sector_id in sectors_ids:\n",
    "        api_url = sectores_api_url+sector_id\n",
    "        response = requests.get(api_url, headers={\"Accept\": \"application/json\"})\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            recursive_func(data['bmx']['cuadros']) \n",
    "        else:\n",
    "            id_cuadros.append('')\n",
    "    \n",
    "    return id_cuadros\n",
    "        \n",
    "\n",
    "cuadros_ids = find_cuadroid_by_sectorid(sector_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuadros_api_url = \"https://www.banxico.org.mx/SieAPIRest/service/v1/cat/estructuras/\"\n",
    "def find_serieid_by_cuadrorid(cuadros_list):\n",
    "    found_series = []\n",
    "    def recursive_func(item):\n",
    "        if isinstance(item, dict):\n",
    "            if 'idSerie' in item:\n",
    "                if (item['idSerie'].strip() != \"\") and (item['idSerie'] not in found_series):\n",
    "                    found_series.append(item['idSerie'])\n",
    "            for value in item.values():\n",
    "                recursive_func(value)\n",
    "        elif isinstance(item, list):\n",
    "            for i_t in item:\n",
    "                recursive_func(i_t)\n",
    "    \n",
    "    for c_id in cuadros_list:\n",
    "        api_url = cuadros_api_url+c_id\n",
    "        response = requests.get(api_url, headers={\"Accept\": \"application/json\"})\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            recursive_func(data['bmx']) \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return found_series\n",
    "                \n",
    "series_ids = find_serieid_by_cuadrorid(cuadros_ids)\n",
    "\n",
    "series_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_series_api(series_list, fechaIni, fechaFin, missing_file, not_processed_path, token):\n",
    "    \n",
    "    if isinstance(series_list, list):\n",
    "        url = f\"https://www.banxico.org.mx/SieAPIRest/service/v1/series/{','.join(series_list)}/datos/{fechaIni}/{fechaFin}\"\n",
    "        print(url)\n",
    "    else:\n",
    "        print(\"Data provided is not a list\")\n",
    "        return None, None\n",
    "    \n",
    "    params = {'token': token}\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            n_call['count'] += 1\n",
    "            data = response.json()\n",
    "            for s_i in data['bmx']['series']:\n",
    "                if 'datos' in s_i.keys():\n",
    "                    pd_series = pd.Series(\n",
    "                                        data=[d['dato'] for d in s_i['datos']],\n",
    "                                        index=pd.to_datetime([d['fecha'] for d in s_i['datos']], format='%d/%m/%Y'),\n",
    "                                        name=s_i['idSerie']\n",
    "                                    )\n",
    "                    pd_series.index.name = 'fecha'\n",
    "                    print(pd_series.tail(3))\n",
    "                    file_name  = os.path.join(database_path, 'banxico_' + s_i['idSerie'] + '.csv')\n",
    "                    if missing_file:\n",
    "                        pd_series.to_csv(file_name)\n",
    "                    else:\n",
    "                        \n",
    "                        old_series = pd.read_csv(file_name, usecols=['fecha', s_i['idSerie']], index_col='fecha').squeeze(axis=1)\n",
    "                        new_series = pd_series[pd_series.index > old_series.index.max()]\n",
    "                        updated_series = pd.concat([old_series, new_series])\n",
    "                        updated_series.to_csv(file_name)\n",
    "                        print(\"Series Updated: \", file_name)\n",
    "\n",
    "            return None, None, False\n",
    "        elif response.status_code == 404:\n",
    "            n_call['count'] += 1\n",
    "            print(\"Non existent series\")\n",
    "            append_to_pickle(not_existent_path, series_list)\n",
    "            return None, None, False\n",
    "        else:\n",
    "            print(response)\n",
    "            print(response.status_code)\n",
    "            return None, None, True\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None, None, True\n",
    "\n",
    "\n",
    "def append_to_pickle(path, data):\n",
    "    if os.path.isfile(path):\n",
    "        with open(path, \"rb\") as fp:\n",
    "            old_data = pickle.load(fp)\n",
    "        old_data.append(data)\n",
    "    else:\n",
    "        old_data = data\n",
    "\n",
    "    with open(path, \"wb\") as fp: \n",
    "        pickle.dump(old_data, fp)\n",
    "    fp.close()\n",
    "\n",
    "\n",
    "def search_series_data(series_list, fechaIni, fechaFin, missing_file, not_processed_path):\n",
    "    clear_output(wait=True)\n",
    "    max_ids = 20\n",
    "    max_retries = 5\n",
    "    initial_delay = 1\n",
    "    for s_idx in range(0, len(series_list), max_ids):\n",
    "        clear_output(wait=True)\n",
    "        series_batch = series_list[s_idx:s_idx + max_ids]\n",
    "        print(\"Processing: \", series_batch)\n",
    "        while tokens[n_call['count']//10_000] != tokens[-1]:\n",
    "            attempt = 1\n",
    "            while attempt <= max_retries:\n",
    "                results, data, response_error = call_series_api(series_batch, fechaIni, fechaFin, missing_file, not_processed_path,token=tokens[n_call['count']//10_000])\n",
    "                if not response_error:\n",
    "                    print(\"Succesfully processed: \", series_batch)\n",
    "                    break\n",
    "                else:\n",
    "                    delay = initial_delay * (2 ** (attempt - 1))\n",
    "                    print(\"Response error, waiting for \", delay)\n",
    "                    time.sleep(delay)\n",
    "                    attempt +=1\n",
    "            else:\n",
    "                print(\"Changing token\")\n",
    "                n_call['count']=+10_000\n",
    "        else:\n",
    "            print(\"Error proccessing: \", series_batch)\n",
    "            append_to_pickle(not_processed_path, series_batch)\n",
    "\n",
    "tokens = [os.getenv(f\"BANXICO_KEY_{i}\") for i in range(1,11)]\n",
    "oldest_date = '1965-01-01'\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "database_path = \"database\"\n",
    "not_processed_path  = \"not_processed\"\n",
    "not_existent_path  = \"not_existent\"\n",
    "batch_size = int(len(series_ids)/10)\n",
    "series_ids_trim = series_ids\n",
    "n_call = {'count': 10_000}\n",
    "\n",
    "\n",
    "if not os.path.isdir('database'):\n",
    "    os.mkdir('database')\n",
    "\n",
    "if os.path.isdir(not_existent_path):\n",
    "    with open(not_existent_path, \"rb\") as fp:\n",
    "        non_existent_series = pickle.load(fp)\n",
    "    fp.close()\n",
    "else:\n",
    "    non_existent_series = []\n",
    "    \n",
    "print(\"non_existent_series: \", non_existent_series)\n",
    "\n",
    "missing_csv = [i for i in series_ids_trim if (not os.path.isfile(os.path.join(database_path, 'banxico_' + i + '.csv'))) and (i not in non_existent_series)]\n",
    "\n",
    "\n",
    "search_series_data(missing_csv, oldest_date, today_date, True, not_processed_path)\n",
    "\n",
    "series_to_update = [i for i in series_ids_trim if i not in missing_csv]\n",
    "print(f\"series_to_update:{len(series_to_update)}: {series_to_update}\")\n",
    "\n",
    "\n",
    "dates = [pd.read_csv(os.path.join(database_path, f'banxico_{s}.csv'))['fecha'] for s in series_to_update]\n",
    "#pd.read_csv(file_name, usecols=['fecha', s_i['idSerie']], index_col='fecha')\n",
    "\n",
    "for i in range(len(dates)):\n",
    "    print(dates[i], series_to_update[i])\n",
    "    print(pd.to_datetime(dates[i]), series_to_update[i])\n",
    "\n",
    "print(\"date: \", min(pd.to_datetime(date).max() for date in dates))\n",
    "last_date_in_data = min(pd.to_datetime(date, format='%Y-%m-%d').max() for date in dates).strftime('%Y-%m-%d')\n",
    "\n",
    "print(\"last_date_in_data: \", last_date_in_data)\n",
    "\n",
    "search_series_data(series_to_update, last_date_in_data, today_date, False, not_processed_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obj_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
